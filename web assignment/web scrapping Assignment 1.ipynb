{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "982765ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Obtaining dependency information for bs4 from https://files.pythonhosted.org/packages/51/bb/bf7aab772a159614954d84aa832c129624ba6c32faa559dfb200a534e50b/bs4-0.0.2-py2.py3-none-any.whl.metadata\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\lib\\site-packages (from bs4) (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.4)\n",
      "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.2\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b565ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#improting Libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b6169c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           Header\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you know ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "#1) Write a python program to display all the header tags from wikipedia.org and make data frame.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Main_Page\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "header_tags = soup.find_all([\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\"])\n",
    "header_texts = [tag.get_text() for tag in header_tags]\n",
    "df = pd.DataFrame(header_texts, columns=[\"Header\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcceaa90",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m names \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     10\u001b[0m terms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m table\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtr\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m     12\u001b[0m     columns \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m     name \u001b[38;5;241m=\u001b[39m columns[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "#2) Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice) \n",
    "# from https://presidentofindia.nic.in/former-presidents.htm and make data frame#\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "page = requests.get(\"https://presidentofindia.nic.in/former-presidents.htm\")\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "table = soup.find(\"table\")\n",
    "names = []\n",
    "terms = []\n",
    "for i in table.find_all(\"tr\")[1:]:\n",
    "    columns = i.find_all(\"td\")\n",
    "    name = columns[0].text.strip()\n",
    "    term = columns[1].text.strip()\n",
    "    names.append(name)\n",
    "    terms.append(term)\n",
    "data = {\"Name\": names, \"Term of Office\": terms}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ee2df84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "Empty DataFrame\n",
      "Columns: [Rank, Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "#a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "#c) Top 10 ODI bowlers along with the records of their team andrating.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape top 10 ODI teams\n",
    "teams_url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "teams_response = requests.get(teams_url)\n",
    "teams_soup = BeautifulSoup(teams_response.content, 'html.parser')\n",
    "\n",
    "team_rows = teams_soup.select('table.table tbody tr')\n",
    "team_data = []\n",
    "for row in team_rows:\n",
    "    rank = row.select_one('.rank-cell').text.strip()\n",
    "    team = row.select_one('.u-hide-phablet').text.strip()\n",
    "    matches = row.select_one('.matches-cell').text.strip()\n",
    "    points = row.select_one('.points-cell').text.strip()\n",
    "    rating = row.select_one('.rating').text.strip()\n",
    "    team_data.append([rank, team, matches, points, rating])\n",
    "\n",
    "teams_df = pd.DataFrame(team_data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "# Scrape top 10 ODI batsmen\n",
    "batsmen_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "batsmen_response = requests.get(batsmen_url)\n",
    "batsmen_soup = BeautifulSoup(batsmen_response.content, 'html.parser')\n",
    "\n",
    "batsman_rows = batsmen_soup.select('table.table tbody tr')\n",
    "batsman_data = []\n",
    "for row in batsman_rows:\n",
    "    rank = row.select_one('.rank-cell').text.strip()\n",
    "    player = row.select_one('.table-body__cell.name a').text.strip()\n",
    "    team = row.select_one('.table-body__cell.u-hide-phablet a').text.strip()\n",
    "    rating = row.select_one('.rating').text.strip()\n",
    "    batsman_data.append([rank, player, team, rating])\n",
    "\n",
    "batsmen_df = pd.DataFrame(batsman_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "# Scrape top 10 ODI bowlers\n",
    "bowlers_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "bowlers_response = requests.get(bowlers_url)\n",
    "bowlers_soup = BeautifulSoup(bowlers_response.content, 'html.parser')\n",
    "\n",
    "bowler_rows = bowlers_soup.select('table.table tbody tr')\n",
    "bowler_data = []\n",
    "for row in bowler_rows:\n",
    "    rank = row.select_one('.rank-cell').text.strip()\n",
    "    player = row.select_one('.table-body__cell.name a').text.strip()\n",
    "    team = row.select_one('.table-body__cell.u-hide-phablet a').text.strip()\n",
    "    rating = row.select_one('.rating').text.strip()\n",
    "    bowler_data.append([rank, player, team, rating])\n",
    "\n",
    "bowlers_df = pd.DataFrame(bowler_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "# Print the dataframes\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(teams_df.head(10))\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(batsmen_df.head(10))\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(bowlers_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "005c11ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI teams in women's cricket:\n",
      "Empty DataFrame\n",
      "Columns: [Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 women's ODI Batting players:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 women's ODI all-rounders:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame-\n",
    "#a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "#b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "#c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Scrape Top 10 ODI teams in women's cricket\n",
    "teams_url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "teams_response = requests.get(teams_url)\n",
    "teams_soup = BeautifulSoup(teams_response.content, 'html.parser')\n",
    "\n",
    "team_rows = teams_soup.select('table.table tbody tr')\n",
    "team_data = []\n",
    "for row in team_rows:\n",
    "    rank = row.select_one('.rank-cell').text.strip()\n",
    "    team = row.select_one('.u-hide-phablet').text.strip()\n",
    "    matches = row.select_one('.matches-cell').text.strip()\n",
    "    points = row.select_one('.points-cell').text.strip()\n",
    "    rating = row.select_one('.rating').text.strip()\n",
    "    team_data.append([rank, team, matches, points, rating])\n",
    "\n",
    "teams_df = pd.DataFrame(team_data, columns=['Rank', 'Team', 'Matches', 'Points', 'Rating'])\n",
    "\n",
    "# Scrape Top 10 women's ODI Batting players\n",
    "batsmen_url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "batsmen_response = requests.get(batsmen_url)\n",
    "batsmen_soup = BeautifulSoup(batsmen_response.content, 'html.parser')\n",
    "\n",
    "batsman_rows = batsmen_soup.select('table.table tbody tr')\n",
    "batsman_data = []\n",
    "for row in batsman_rows:\n",
    "    rank = row.select_one('.rank-cell').text.strip()\n",
    "    player = row.select_one('.table-body__cell.name a').text.strip()\n",
    "    team = row.select_one('.table-body__cell.u-hide-phablet a').text.strip()\n",
    "    rating = row.select_one('.rating').text.strip()\n",
    "    batsman_data.append([rank, player, team, rating])\n",
    "\n",
    "batsmen_df = pd.DataFrame(batsman_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "\n",
    "# Scrape Top 10 women's ODI all-rounders\n",
    "allrounder_url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/allrounder'\n",
    "allrounder_response = requests.get(allrounder_url)\n",
    "allrounder_soup = BeautifulSoup(allrounder_response.content, 'html.parser')\n",
    "\n",
    "allrounder_rows = allrounder_soup.select('table.table tbody tr')\n",
    "allrounder_data = []\n",
    "for row in allrounder_rows:\n",
    "    rank = row.select_one('.rank-cell').text.strip()\n",
    "    player = row.select_one('.table-body__cell.name a').text.strip()\n",
    "    team = row.select_one('.table-body__cell.u-hide-phablet a').text.strip()\n",
    "    rating = row.select_one('.rating').text.strip()\n",
    "    allrounder_data.append([rank, player, team, rating])\n",
    "\n",
    "allrounder_df = pd.DataFrame(bowler_data, columns=['Rank', 'Player', 'Team', 'Rating'])\n",
    "\n",
    "\n",
    "# Create data frames\n",
    "df_teams = pd.DataFrame(teams_data, columns=[\"Team\", \"Matches\", \"Points\", \"Rating\"])\n",
    "df_batting = pd.DataFrame(batting_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "df_allrounders = pd.DataFrame(allrounders_data, columns=[\"Player\", \"Team\", \"Rating\"])\n",
    "\n",
    "# Print the data frames\n",
    "print(\"Top 10 ODI teams in women's cricket:\")\n",
    "print(df_teams)\n",
    "print(\"\\nTop 10 women's ODI Batting players:\")\n",
    "print(df_batting)\n",
    "print(\"\\nTop 10 women's ODI all-rounders:\")\n",
    "print(df_allrounders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df50bf1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (800455691.py, line 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[24], line 20\u001b[1;36m\u001b[0m\n\u001b[1;33m    headline.append(headline)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and make data frame-\n",
    "#i) Headline\n",
    "#ii) Time\n",
    "#iii) News Link\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.cnbc.com/world/?region=world\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "articles = soup.find_all(\"div\", class_=\"Card-titleContainer\")\n",
    "headlines = []\n",
    "times = []\n",
    "links = []\n",
    "\n",
    "for article in articles:\n",
    "  # Extract the headline\n",
    "  headline = article.find(\"a\").text.strip()\n",
    "    headlines.append(headline)\n",
    "  \n",
    "  # Extract the time\n",
    "    time = article.find(\"time\").text.strip()\n",
    "    times.append(time)\n",
    "  \n",
    "  # Extract the news link\n",
    "    link = article.find(\"a\")[\"href\"]\n",
    "    links.append(link)\n",
    "    \n",
    "data = {\n",
    "  \"Headline\": headlines,\n",
    "  \"Time\": times,\n",
    "  \"News Link\": links\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e9b5f1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 23)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m<tokenize>:23\u001b[1;36m\u001b[0m\n\u001b[1;33m    titles.append(title)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "#6) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles Scrape below mentioned details and make data frame-\n",
    "#i) Paper Title\n",
    "#ii) Authors\n",
    "#iii) Published Date\n",
    "#iv) Paper URL\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "response = requests.get(\"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\")\n",
    "\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "articles_container = soup.find(\"div\", class_=\"pod-listing\")\n",
    "\n",
    "titles = []\n",
    "authors = []\n",
    "dates = []\n",
    "urls = []\n",
    "\n",
    "for article in articles_container.find_all(\"li\"):\n",
    "     title = article.find(\"h3\").text.strip()\n",
    "    titles.append(title)\n",
    "  \n",
    "    author = article.find(\"span\", class_=\"text-xs\").text.strip()\n",
    "    authors.append(author)\n",
    "  \n",
    "    date = article.find(\"span\", class_=\"text-xs\").find_next_sibling(\"span\").text.strip()\n",
    "    dates.append(date)\n",
    "  \n",
    "    url = article.find(\"a\")[\"href\"]\n",
    "    urls.append(url)\n",
    "\n",
    "data = {\n",
    "  \"Paper Title\": titles,\n",
    "  \"Authors\": authors,\n",
    "  \"Published Date\": dates,\n",
    "  \"Paper URL\": urls\n",
    "}\n",
    "df = pd.DataFrame(dataframe)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "510cfe0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#7) Write a python program to scrape mentioned details from dineout.co.in and make data frame-\n",
    "#i) Restaurant name\n",
    "#ii) Cuisine\n",
    "#iii) Location\n",
    "#iv) Ratings\n",
    "#v) Image URL\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://www.dineout.co.in\"\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "restaurant_names = soup.find_all('h2', class_='restnt-name ellipsis')\n",
    "cuisines = soup.find_all('span', class_='double-line-ellipsis')\n",
    "locations = soup.find_all('span', class_='double-line-ellipsis')\n",
    "ratings = soup.find_all('span', class_='rating-value')\n",
    "image_urls = soup.find_all('img', class_='img-responsive')\n",
    "restaurant_list = []\n",
    "cuisine_list = []\n",
    "location_list = []\n",
    "rating_list = []\n",
    "image_url_list = []\n",
    "\n",
    "for name in restaurant_names:\n",
    "    restaurant_list.append(name.text.strip())\n",
    "\n",
    "for cuisine in cuisines:\n",
    "    cuisine_list.append(cuisine.text.strip())\n",
    "\n",
    "for location in locations:\n",
    "    location_list.append(location.text.strip())\n",
    "\n",
    "for rating in ratings:\n",
    "    rating_list.append(rating.text.strip())\n",
    "\n",
    "for image in image_urls:\n",
    "    image_url_list.append(image['src'])\n",
    "\n",
    "data = {\n",
    "  'Restaurant Name': restaurant_list,\n",
    "  'Cuisine': cuisine_list,\n",
    "  'Location': location_list,\n",
    "  'Ratings': rating_list,\n",
    "  'Image URL': image_url_list\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c8fd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
